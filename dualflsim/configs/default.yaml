# DualTF-FLSim default configuration
# Settings for Ray, based on the server condition.
ray:
  num_cpus: 32
  num_gpus: 4
  ignore_reinit_error: true

resources:
  client:
    num_cpus: 4
    num_gpus: 1

simulation:
  num_clients: 28
  total_rounds: 2
  fraction_fit: 0.15
  fraction_evaluate: 0.0
  min_available_clients: 4
  min_fit_clients: 4

strategy:
  # Choose between 'scaffold' and 'fedprox'
  type: scaffold

training:
  local_epochs: 5
  lr: 1.0e-4
  proximal_mu: 0.0
  log_step_recon_stats: true   # If true, record per-step reconstruction stats (mean/std/min/max/last)

model:
  # seq_len MUST equal data.seq_length
  seq_len: 75
  time:
    enc_in: 38
    c_out: 38
    e_layers: 3
    n_heads: 8
    d_model: 512
  freq:
    # nest_length is used for data grand-windowing and the freq model window size
    nest_length: 25
    enc_in: 38
    c_out: 38
    e_layers: 3
    n_heads: 4
    d_model: 512

data:
  dataset: SMD  # One of: PSM, SMD, SMAP, MSL
  # Federated partitioning strategy:
  # - sequential: contiguous time slices (PSM)
  # - by_machine: SMAP, MSL, SMD
  partition_mode: by_machine      # sequential | by_machine
  time_batch_size: 64
  freq_batch_size: 32
  # seq_length MUST equal model.seq_len
  seq_length: 75
  step: 1
  # Centralized caching (loads dataset once on server, broadcasts indices)
  centralized_cache: true        # Set false to fall back to legacy per-client full load
  cache_dir: .cache/dataset      # Relative path where cache arrays (x_train/x_test/y_test) are stored
  cache_clients: null            # Optional override for number of sequential slices cached
  max_train_windows_per_client: null  # Optional cap per client (None for all)
  client_load_test: false             # If false, only server loads test set; clients load only their train slice

evaluation: # Basically we don't need this, so keep it false.
  enabled: false   # Keep false for by_machine minimal server role
  min_consecutive: 10
  num_thresholds_periodic: 256
  num_thresholds_final: 1000
  seq_length: 50
  step: 5
  threshold_quantile_range: [0.01, 0.99]
  prefer_gpu_sweep: true

scaffold:
  damping: 0.1
  max_corr_norm: 1.0
  grad_clip: 1.0

post_training:
  generate_arrays: true
  dataset: SMD   # Must match data.dataset for consistency
  data_num: 0 # if you wanna run different experiment on same dataset, use different data_num.
  seq_length: 75
  nest_length: 25
  build_server_test_after_training: true  # If true and by_machine mode, server will load all test files only after training for array generation, keep it true for by_machine mode.

wandb: # I didn't change the wandb logic, so maybe this part can be refined, just maybe (it takes time) 
  enabled: False 
  project: DualTF-FLSim
  entity: null      # optional
  run_name: SMD_iid_2rounds # optional; autogenerated if null
  tags: [fl, dualtf, smd]
  notes: ""
  log_per_client: true           # log per-client loss/time/memory each round
  log_gpu_memory: true           # log server GPU memory snapshots
  log_round_timing: true         # log round duration (s)
  log_total_sim_time: true       # log total simulation wall time
  log_parameter_norms: false     # optionally log global weight norms
  log_comm_size: false           # log aggregated parameter sizes (bytes)
  max_logged_clients: 32         # cap per-round per-client metric logging
  histogram_per_client_loss: false  # log histogram of client losses per round
