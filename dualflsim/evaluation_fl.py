"""FL-Compatible Evaluation Script for DualTF-FLSim.

This script provides the same evaluation functionality as the original DualTF
evaluation.py but is compatible with arrays generated by the federated learning
setup. It loads the pickle files generated by array_generator.py and performs
comprehensive evaluation using the same methodology as the original.
"""

import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm
import argparse
import sys
from pathlib import Path

from sklearn.metrics import auc
from sklearn.preprocessing import RobustScaler

# Add the project root to Python path for imports
project_root = str(Path(__file__).resolve().parents[1])
if project_root not in sys.path:
    sys.path.append(project_root)

from utils.data_loader import load_PSM, normalization, _create_sequences


def _simulate_thresholds(rec_errors, n):
    """Generate threshold values for evaluation (copied from original)."""
    thresholds, step_size = [], abs(np.max(rec_errors) - np.min(rec_errors)) / n
    th = np.min(rec_errors)
    thresholds.append(th)

    print(f'Threshold Range: ({np.min(rec_errors)}, {np.max(rec_errors)}) with Step Size: {step_size}')
    for i in range(n):
        th = th + step_size
        thresholds.append(th)

    return thresholds


def load_evaluation_arrays(dataset="PSM", form=None, data_num=0):
    """Load time and frequency evaluation arrays from pickle files."""

    # Time array
    print('Time Arrays Loading...')
    if dataset == 'NeurIPSTS' and form is not None:
        time_file_path = f'./time_arrays/{dataset}_{form}_time_evaluation_array.pkl'
    else:
        time_file_path = f'./time_arrays/{dataset}_{data_num}_time_evaluation_array.pkl'

    try:
        time_array = pd.read_pickle(time_file_path)
        print(f"Time array loaded: {time_array.shape}")
        print("Time array indices:", list(time_array.index))
    except FileNotFoundError:
        print(f"Error: Time array file not found at {time_file_path}")
        print("Please run FL training first to generate the arrays.")
        return None, None

    # Frequency array
    print('Frequency Arrays Loading...')
    if dataset == 'NeurIPSTS' and form is not None:
        freq_file_path = f'./freq_arrays/{dataset}_{form}_freq_evaluation_array.pkl'
    else:
        freq_file_path = f'./freq_arrays/{dataset}_{data_num}_freq_evaluation_array.pkl'

    try:
        freq_array = pd.read_pickle(freq_file_path)
        print(f"Freq array loaded: {freq_array.shape}")
        print("Freq array indices:", list(freq_array.index))
    except FileNotFoundError:
        print(f"Error: Freq array file not found at {freq_file_path}")
        print("Please run FL training first to generate the arrays.")
        return None, None

    return time_array, freq_array


def total_evaluation(opts):
    """Main evaluation function (adapted from original evaluation.py)."""

    # Load arrays
    time_array, freq_array = load_evaluation_arrays(
        dataset=opts.dataset,
        form=getattr(opts, 'form', None),
        data_num=getattr(opts, 'data_num', 0)
    )

    if time_array is None or freq_array is None:
        return

    print("Time Array:")
    print(time_array)
    print("\nFreq Array:")
    print(freq_array)

    # Extract reconstruction errors for fusion
    time_rec = np.array(time_array.loc['Avg(RE)', :])
    freq_rec = np.array(freq_array.loc['Avg(exp(RE))', :])

    time_rec = time_rec.reshape(-1, 1)
    freq_rec = freq_rec.reshape(-1, 1)

    # Normalize scores using RobustScaler (as in original)
    scaler = RobustScaler(unit_variance=True)
    time_as = scaler.fit_transform(time_rec)
    freq_as = scaler.transform(freq_rec)

    # Also apply normalization function (as in original)
    time_as = normalization(time_rec)
    freq_as = normalization(freq_rec)

    # Fusion: combine time and frequency scores
    final_as = time_as + freq_as
    final_as = final_as.flatten()  # Ensure it's 1D

    # Load ground truth labels
    print("Loading ground truth labels...")
    if opts.dataset == 'PSM':
        data_dict = load_PSM(seq_length=opts.seq_length, stride=1)
        label = data_dict['y_test'][0]  # Get labels for data_num=0
    else:
        print(f"Dataset {opts.dataset} not implemented yet")
        return

    # Ensure labels and scores have compatible lengths
    min_len = min(len(final_as), len(label))
    final_as = final_as[:min_len]
    label = label[:min_len]

    print(f"Final evaluation lengths - Scores: {len(final_as)}, Labels: {len(label)}")

    # === Point Adjusted Evaluation ===
    pa_scores = {'dataset': [], 'f1': [], 'precision': [], 'recall': [], 'pr_auc': [], 'roc_auc': []}
    print('##### Point Adjusted Evaluation #####')
    thresholds = _simulate_thresholds(final_as, opts.thresh_num)
    final_as_seq = _create_sequences(final_as, opts.seq_length, opts.step)

    labels = _create_sequences(label, opts.seq_length, opts.step)

    TP, TN, FP, FN = [], [], [], []
    precision, recall, f1, fpr = [], [], [], []
    for th in tqdm(thresholds): # for each threshold
        TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
        for t in range(len(final_as_seq)): # for each sequence

            # if any part of the segment has an anomaly, we consider it as anomalous sequence
            true_anomalies, pred_anomalies = set(np.where(labels[t] == 1)[0]), set(np.where(final_as_seq[t] > th)[0])

            if len(pred_anomalies) > 0 and len(pred_anomalies.intersection(true_anomalies)) > 0:
                # correct prediction (at least partial overlap with true anomalies)
                TP_t = TP_t + 1
            elif len(pred_anomalies) == 0 and len(true_anomalies) == 0:
                # correct rejection, no predicted anomaly on no true labels
                TN_t = TN_t + 1
            elif len(pred_anomalies) > 0 and len(true_anomalies) == 0:
                # false alarm (i.e., predict anomalies on no true labels)
                FP_t = FP_t + 1
            elif len(pred_anomalies) == 0 and len(true_anomalies) > 0:
                # predict no anomaly when there is at least one true anomaly within the seq.
                FN_t = FN_t + 1

        TP.append(TP_t)
        TN.append(TN_t)
        FP.append(FP_t)
        FN.append(FN_t)

    for i in range(len(thresholds)):
        precision.append(TP[i] / (TP[i] + FP[i] + 1e-7))
        recall.append(TP[i] / (TP[i] + FN[i] + 1e-7)) # recall or true positive rate (TPR)
        fpr.append(FP[i] / (FP[i] + TN[i] + 1e-7))
        f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-7))

    highest_th_idx = np.argmax(f1)
    print(f'Threshold: {thresholds[highest_th_idx]}')
    print("Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} ".format(
            precision[highest_th_idx], recall[highest_th_idx], f1[highest_th_idx]))
    print("PR-AUC : {:0.4f}, ROC-AUC : {:0.4f}".format(auc(recall, precision), auc(fpr, recall)))

    pa_scores['dataset'].append(f'{opts.dataset}_{opts.data_num}')
    pa_scores['f1'].append(f1[highest_th_idx])
    pa_scores['precision'].append(precision[highest_th_idx])
    pa_scores['recall'].append(recall[highest_th_idx])
    pa_scores['pr_auc'].append(auc(recall, precision))
    pa_scores['roc_auc'].append(auc(fpr, recall))
    results = pd.DataFrame(pa_scores)
    print("Point Adjusted Results:")
    print(results)

    # === Point-Wise Evaluation ===
    pw_scores = {'dataset': [], 'f1': [], 'precision': [], 'recall': [], 'pr_auc': [], 'roc_auc': []}
    print('\n##### Point-Wise Evaluation #####')
    TP, TN, FP, FN = [], [], [], []
    precision, recall, f1, fpr = [], [], [], []
    for th in tqdm(thresholds): # for each threshold

        TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
        for ts in range(len(final_as)):
            if label[ts] == 1:
                if final_as[ts] >= th:
                    TP_t = TP_t + 1
                elif final_as[ts] < th:
                    FN_t = FN_t + 1
            elif label[ts] == 0:
                if final_as[ts] >= th:
                    FP_t = FP_t + 1
                elif final_as[ts] < th:
                    TN_t = TN_t + 1

        TP.append(TP_t)
        TN.append(TN_t)
        FP.append(FP_t)
        FN.append(FN_t)

    for i in range(len(thresholds)):
        precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
        recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
        fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
        f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))

    highest_th_idx = np.argmax(f1)
    print(f'Threshold: {thresholds[highest_th_idx]}')
    print("Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} ".format(
            precision[highest_th_idx], recall[highest_th_idx], f1[highest_th_idx]))
    print("PR-AUC : {:0.4f}, ROC-AUC : {:0.4f}".format(auc(recall, precision), auc(fpr, recall)))

    pw_scores['dataset'].append(f'{opts.dataset}_{opts.data_num}')
    pw_scores['f1'].append(f1[highest_th_idx])
    pw_scores['precision'].append(precision[highest_th_idx])
    pw_scores['recall'].append(recall[highest_th_idx])
    pw_scores['pr_auc'].append(auc(recall, precision))
    pw_scores['roc_auc'].append(auc(fpr, recall))
    results = pd.DataFrame(pw_scores)
    print("Point-Wise Results:")
    print(results)

    # === Released Point-Wise Evaluation (with nest_length) ===
    print('\n##### Released Point-Wise Evaluation #####')
    pr_scores = {'dataset': [], 'f1': [], 'precision': [], 'recall': [], 'pr_auc': [], 'roc_auc': []}
    thresholds = _simulate_thresholds(final_as, opts.thresh_num)
    final_as_seq = _create_sequences(final_as, opts.nest_length, opts.step)
    labels = _create_sequences(label, opts.nest_length, opts.step)

    TP, TN, FP, FN = [], [], [], []
    precision, recall, f1, fpr = [], [], [], []
    for th in tqdm(thresholds): # for each threshold
        TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
        for t in range(len(final_as_seq)): # for each sequence

            # if any part of the segment has an anomaly, we consider it as anomalous sequence
            true_anomalies, pred_anomalies = set(np.where(labels[t] == 1)[0]), set(np.where(final_as_seq[t] > th)[0])

            if len(pred_anomalies) > 0 and len(pred_anomalies.intersection(true_anomalies)) > 0:
                # correct prediction (at least partial overlap with true anomalies)
                TP_t = TP_t + 1
            elif len(pred_anomalies) == 0 and len(true_anomalies) == 0:
                # correct rejection, no predicted anomaly on no true labels
                TN_t = TN_t + 1
            elif len(pred_anomalies) > 0 and len(true_anomalies) == 0:
                # false alarm (i.e., predict anomalies on no true labels)
                FP_t = FP_t + 1
            elif len(pred_anomalies) == 0 and len(true_anomalies) > 0:
                # predict no anomaly when there is at least one true anomaly within the seq.
                FN_t = FN_t + 1

        TP.append(TP_t)
        TN.append(TN_t)
        FP.append(FP_t)
        FN.append(FN_t)

    for i in range(len(thresholds)):
        precision.append(TP[i] / (TP[i] + FP[i] + 1e-7))
        recall.append(TP[i] / (TP[i] + FN[i] + 1e-7)) # recall or true positive rate (TPR)
        fpr.append(FP[i] / (FP[i] + TN[i] + 1e-7))
        f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-7))

    highest_th_idx = np.argmax(f1)
    print(f'Threshold: {thresholds[highest_th_idx]}')
    print("Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} ".format(
            precision[highest_th_idx], recall[highest_th_idx], f1[highest_th_idx]))
    print("PR-AUC : {:0.4f}, ROC-AUC : {:0.4f}".format(auc(recall, precision), auc(fpr, recall)))

    pr_scores['dataset'].append(f'{opts.dataset}_{opts.data_num}')
    pr_scores['f1'].append(f1[highest_th_idx])
    pr_scores['precision'].append(precision[highest_th_idx])
    pr_scores['recall'].append(recall[highest_th_idx])
    pr_scores['pr_auc'].append(auc(recall, precision))
    pr_scores['roc_auc'].append(auc(fpr, recall))
    results = pd.DataFrame(pr_scores)
    print("Released Point-Wise Results:")
    print(results)

    print("\n=== FL Evaluation Complete ===")


def main():
    """Main entry point for FL evaluation."""
    parser = argparse.ArgumentParser(description='FL-Compatible Evaluation for DualTF')

    # Settings
    parser.add_argument('--thresh_num', type=int, default=1000,
                        help='Number of thresholds for evaluation')
    parser.add_argument('--seq_length', type=int, default=100,
                        help='Sequence length used during training')
    parser.add_argument('--nest_length', type=int, default=25,
                        help='Nested sequence length')
    parser.add_argument('--step', type=int, default=1,
                        help='Step size for sequence creation')
    parser.add_argument('--dataset', type=str, default='PSM',
                        help='Dataset name')
    parser.add_argument('--form', type=str, default=None,
                        help='Form for NeurIPSTS dataset')
    parser.add_argument('--data_num', type=int, default=0,
                        help='Data number')

    opts = parser.parse_args()

    # Display settings
    if opts.dataset == 'NeurIPSTS' and opts.form:
        print(f"Dataset: {opts.dataset}\nForm: {opts.form}\nSeq_length: {opts.seq_length}\nNest_length: {opts.nest_length}")
    else:
        print(f"Dataset: {opts.dataset}\nNum: {opts.data_num}\nSeq_length: {opts.seq_length}\nNest_length: {opts.nest_length}")

    # Run evaluation
    total_evaluation(opts)


if __name__ == '__main__':
    main()