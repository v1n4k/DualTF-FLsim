"""FL-Compatible Evaluation Script for DualTF-FLSim.

This script provides the same evaluation functionality as the original DualTF
evaluation.py but is compatible with arrays generated by the federated learning
setup. It loads the pickle files generated by array_generator.py and performs
comprehensive evaluation using the same methodology as the original.
"""

import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm
import argparse
import sys
from pathlib import Path
import math
import sklearn

from sklearn.metrics import auc
from sklearn.preprocessing import RobustScaler

# Add the project root to Python path for imports
project_root = str(Path(__file__).resolve().parents[1])
if project_root not in sys.path:
    sys.path.append(project_root)

from utils.data_loader import load_PSM, normalization, _create_sequences, load_dataset_by_name


# === TranAD-style adjusted prediction utilities (adapted from FedKO/utils/ad_util.py) ===
def adjust_predicts_from_tranad(label, score, threshold=None, pred=None, calc_latency=False):
    """
    Calculate adjusted predict labels using given score/threshold (or given pred) and label.
    This expands predicted positives to cover entire contiguous anomaly segments, reducing latency.
    """
    if len(score) != len(label):
        raise ValueError("score and label must have the same length")
    score = np.asarray(score)
    label = np.asarray(label)
    latency = 0
    if pred is None:
        predict = score > threshold
    else:
        predict = pred.copy()
    actual = label > 0.1
    anomaly_state = False
    anomaly_count = 0
    for i in range(len(score)):
        if actual[i] and predict[i] and not anomaly_state:
            anomaly_state = True
            anomaly_count += 1
            for j in range(i, 0, -1):
                if not actual[j]:
                    break
                else:
                    if not predict[j]:
                        predict[j] = True
                        latency += 1
        elif not actual[i]:
            anomaly_state = False
        if anomaly_state:
            predict[i] = True
    if calc_latency:
        return predict, latency / (anomaly_count + 1e-4)
    else:
        return predict


def get_threshold_tranad(labels, scores, print_or_not=True):
    """
    Sweep a sparse set of thresholds, compute raw and TranAD-adjusted point-wise metrics,
    and return the best metrics (by adjusted F1) together with ROC-AUC.
    """
    auc_val = sklearn.metrics.roc_auc_score(labels, scores)
    thresholds_0 = np.asarray(scores).copy()
    thresholds_0.sort()

    thresholds = []
    for i in range(len(thresholds_0)):
        if i % 1000 == 0 or i == len(thresholds_0) - 1:
            thresholds.append(thresholds_0[i])

    best_precision = 0.0
    best_recall = 0.0
    best_f1 = 0.0
    best_threshold = math.inf
    best_f1_adjusted = 0.0
    best_precision_adjusted = 0.0
    best_recall_adjusted = 0.0

    labels_np = np.asarray(labels).astype(int)
    scores_np = np.asarray(scores).astype(float)

    for threshold in thresholds:
        y_pred_from_threshold = (scores_np >= threshold).astype(int)
        precision = sklearn.metrics.precision_score(labels_np, y_pred_from_threshold, zero_division=0)
        recall = sklearn.metrics.recall_score(labels_np, y_pred_from_threshold, zero_division=0)
        f1 = sklearn.metrics.f1_score(labels_np, y_pred_from_threshold, zero_division=0)

        y_pred_adjusted = adjust_predicts_from_tranad(labels_np, scores_np, pred=y_pred_from_threshold, threshold=threshold)
        precision_adjusted = sklearn.metrics.precision_score(labels_np, y_pred_adjusted, zero_division=0)
        recall_adjusted = sklearn.metrics.recall_score(labels_np, y_pred_adjusted, zero_division=0)
        f1_adjusted = sklearn.metrics.f1_score(labels_np, y_pred_adjusted, zero_division=0)

        if f1_adjusted > best_f1_adjusted:
            best_precision = precision
            best_recall = recall
            best_f1 = f1
            best_f1_adjusted = f1_adjusted
            best_precision_adjusted = precision_adjusted
            best_recall_adjusted = recall_adjusted
            best_threshold = threshold

    if print_or_not:
        print('auc:', auc_val)
        print('precision_adjusted:', best_precision_adjusted)
        print('recall_adjusted:', best_recall_adjusted)
        print('f1:', best_f1)
        print('f1_adjusted:', best_f1_adjusted)
        print('threshold:', best_threshold)

    return (
        auc_val,
        best_precision,
        best_recall,
        best_f1,
        best_precision_adjusted,
        best_recall_adjusted,
        best_f1_adjusted,
        best_threshold,
    )


def _generate_thresholds(scores, max_n, strategy='quantile'):
    """Generate an informative, reduced set of thresholds.

    Strategy:
      - If unique score count <= max_n: return all unique sorted scores (exact sweep)
      - Else if strategy == 'quantile': sample max_n quantiles (uniform in [0,1])
      - Fallback: uniform numeric spacing (legacy behavior)

    This reduces redundant thresholds when many repeated scores exist and
    accelerates evaluation substantially for large T.
    """
    scores = np.asarray(scores).ravel()
    if scores.size == 0:
        return [0.0]

    unique_scores = np.unique(scores)
    if unique_scores.size == 1:
        # All scores identical -> only one meaningful threshold
        return unique_scores.tolist()

    if unique_scores.size <= max_n:
        thresholds = unique_scores
        print(f"[ThresholdGen] Using all {thresholds.size} unique scores (<= max_n={max_n}).")
    else:
        if strategy == 'quantile':
            qs = np.linspace(0.0, 1.0, num=max_n)
            thresholds = np.quantile(scores, qs, method='linear')
            thresholds = np.unique(thresholds)
            print(f"[ThresholdGen] Quantile sampling produced {thresholds.size} thresholds (requested {max_n}).")
        else:
            mn, mx = scores.min(), scores.max()
            step = (mx - mn) / max_n
            thresholds = mn + step * np.arange(max_n + 1)
            print(f"[ThresholdGen] Uniform stepping produced {thresholds.size} thresholds (range=({mn},{mx})).")

    return thresholds.tolist()

# === Optimization Notes ===
# Threshold generation now samples either all unique scores (if <= max_n) or
# a quantile-based subset. This removes redundant thresholds coming from
# dense score regions and typically shrinks threshold count dramatically.
#
# Point-wise evaluation was refactored from O(T * K) loops (iterating all T
# points for each of K thresholds) to O(T log T + K log T). We sort scores
# once (O(T log T)) then for each threshold perform a binary search
# (np.searchsorted) plus O(1) arithmetic with cumulative positive counts.
# For large T this yields substantial speedups while producing identical
# metrics compared to exhaustive looping over the same threshold set.

# (Optional) Legacy point-wise evaluator kept for manual validation.
# def _legacy_pointwise_metrics(scores, labels, thresholds):
#     TP = []; TN = []; FP = []; FN = []
#     precision = []; recall = []; f1 = []; fpr = []
#     for th in thresholds:
#         TP_t = TN_t = FP_t = FN_t = 0
#         for i in range(len(scores)):
#             if labels[i] == 1:
#                 if scores[i] >= th:
#                     TP_t += 1
#                 else:
#                     FN_t += 1
#             else:
#                 if scores[i] >= th:
#                     FP_t += 1
#                 else:
#                     TN_t += 1
#         TP.append(TP_t); TN.append(TN_t); FP.append(FP_t); FN.append(FN_t)
#     for i in range(len(thresholds)):
#         p = TP[i] / (TP[i] + FP[i] + 1e-8)
#         r = TP[i] / (TP[i] + FN[i] + 1e-8)
#         precision.append(p); recall.append(r)
#         f1.append(2 * (p * r) / (p + r + 1e-8))
#         fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
#     return precision, recall, f1, fpr


def load_evaluation_arrays(dataset="PSM", form=None, data_num=0):
    """Load time and frequency evaluation arrays from pickle files."""

    # Time array
    print('Time Arrays Loading...')
    if dataset == 'NeurIPSTS' and form is not None:
        time_file_path = f'./time_arrays/{dataset}_{form}_time_evaluation_array.pkl'
    else:
        time_file_path = f'./time_arrays/{dataset}_{data_num}_time_evaluation_array.pkl'

    try:
        time_array = pd.read_pickle(time_file_path)
        print(f"Time array loaded: {time_array.shape}")
        print("Time array indices:", list(time_array.index))
    except FileNotFoundError:
        print(f"Error: Time array file not found at {time_file_path}")
        print("Please run FL training first to generate the arrays.")
        return None, None

    # Frequency array
    print('Frequency Arrays Loading...')
    if dataset == 'NeurIPSTS' and form is not None:
        freq_file_path = f'./freq_arrays/{dataset}_{form}_freq_evaluation_array.pkl'
    else:
        freq_file_path = f'./freq_arrays/{dataset}_{data_num}_freq_evaluation_array.pkl'

    try:
        freq_array = pd.read_pickle(freq_file_path)
        print(f"Freq array loaded: {freq_array.shape}")
        print("Freq array indices:", list(freq_array.index))
    except FileNotFoundError:
        print(f"Error: Freq array file not found at {freq_file_path}")
        print("Please run FL training first to generate the arrays.")
        return None, None

    return time_array, freq_array


def total_evaluation(opts):
    """Main evaluation function (adapted and extended).

    Now supports evaluating multiple modes:
    - time-only (normalized time reconstruction errors)
    - freq-only (normalized freq exp(RE))
    - fused (sum of normalized time and freq)
    And for each mode, computes:
    - Point Adjusted Evaluation (seq_length windows)
    - Point-Wise Evaluation
    - Released Point-Wise Evaluation (nest_length windows)
    - TranAD-Adjusted Point-Wise Evaluation
    Optionally writes a consolidated CSV to results_dir.
    """

    # Load arrays
    time_array, freq_array = load_evaluation_arrays(
        dataset=opts.dataset,
        form=getattr(opts, 'form', None),
        data_num=getattr(opts, 'data_num', 0)
    )

    if time_array is None or freq_array is None:
        return

    if opts.verbose:
        print("Time Array:")
        print(time_array)
        print("\nFreq Array:")
        print(freq_array)

    # Extract reconstruction errors for fusion
    time_rec = np.array(time_array.loc['Avg(RE)', :]).reshape(-1, 1)
    freq_rec = np.array(freq_array.loc['Avg(exp(RE))', :]).reshape(-1, 1)

    # Align lengths if mismatched (original DualTF expects equal length from PKLs)
    if len(time_rec) != len(freq_rec):
        longer = len(time_rec) if len(time_rec) >= len(freq_rec) else len(freq_rec)
        shorter_name = 'freq' if len(freq_rec) < len(time_rec) else 'time'
        print(f"[WARN] Length mismatch between time ({len(time_rec)}) and freq ({len(freq_rec)}) arrays. Resampling {shorter_name} to length {longer} to align with original evaluation.")
        if len(freq_rec) < len(time_rec):
            # Resample freq to time length
            x_old = np.linspace(0.0, 1.0, len(freq_rec))
            x_new = np.linspace(0.0, 1.0, len(time_rec))
            freq_rec = np.interp(x_new, x_old, freq_rec.ravel()).reshape(-1, 1)
        else:
            # Resample time to freq length (unlikely for PSM, but handle generically)
            x_old = np.linspace(0.0, 1.0, len(time_rec))
            x_new = np.linspace(0.0, 1.0, len(freq_rec))
            time_rec = np.interp(x_new, x_old, time_rec.ravel()).reshape(-1, 1)

    # Normalize scores using RobustScaler (as in original)
    scaler = RobustScaler(unit_variance=True)
    time_as = scaler.fit_transform(time_rec)
    freq_as = scaler.transform(freq_rec)

    # Also apply normalization function (as in original)
    time_as = normalization(time_rec)
    freq_as = normalization(freq_rec)

    # Fusion: combine time and frequency scores
    final_as = (time_as + freq_as).flatten()  # fused

    # Also prepare per-source modes
    time_only_as = time_as.flatten()
    freq_only_as = freq_as.flatten()

    # Load ground truth labels
    print("Loading ground truth labels...")
    if opts.dataset in ('PSM','SMD','SMAP','MSL'):
        # Generic dispatcher
        data_dict = load_dataset_by_name(opts.dataset, seq_length=opts.seq_length, stride=1)
        label = data_dict['y_test'][0]  # Get labels for data_num=0
    else:
        print(f"Dataset {opts.dataset} not implemented yet")
        return

    # Helper to evaluate a single score vector with all methods
    def evaluate_scores(scores_1d: np.ndarray, labels_1d: np.ndarray):
        out = {}
        # Align lengths
        n = min(len(scores_1d), len(labels_1d))
        s = scores_1d[:n]
        y = labels_1d[:n]

        # Point Adjusted (seq_length)
        thresholds = _generate_thresholds(s, opts.thresh_num, strategy='quantile')
        s_seq = _create_sequences(s, opts.seq_length, opts.step)
        y_seq = _create_sequences(y, opts.seq_length, opts.step)
        TP, TN, FP, FN = [], [], [], []
        precision, recall, f1, fpr = [], [], [], []
        for th in thresholds:
            TP_t = TN_t = FP_t = FN_t = 0
            for t in range(len(s_seq)):
                true_anom = set(np.where(y_seq[t] == 1)[0])
                pred_anom = set(np.where(s_seq[t] > th)[0])
                if len(pred_anom) > 0 and len(pred_anom.intersection(true_anom)) > 0:
                    TP_t += 1
                elif len(pred_anom) == 0 and len(true_anom) == 0:
                    TN_t += 1
                elif len(pred_anom) > 0 and len(true_anom) == 0:
                    FP_t += 1
                elif len(pred_anom) == 0 and len(true_anom) > 0:
                    FN_t += 1
            TP.append(TP_t); TN.append(TN_t); FP.append(FP_t); FN.append(FN_t)
        for i in range(len(thresholds)):
            p = TP[i] / (TP[i] + FP[i] + 1e-7)
            r = TP[i] / (TP[i] + FN[i] + 1e-7)
            prc = p; rec = r
            f1_i = 2 * (p * r) / (p + r + 1e-7)
            precision.append(prc); recall.append(rec); f1.append(f1_i)
            fpr.append(FP[i] / (FP[i] + TN[i] + 1e-7))
        idx = int(np.argmax(f1))
        out['point_adjusted'] = {
            'threshold': float(thresholds[idx]),
            'precision': float(precision[idx]),
            'recall': float(recall[idx]),
            'f1': float(f1[idx]),
            'pr_auc': float(auc(recall, precision)),
            'roc_auc': float(auc(fpr, recall)),
        }

        # Point-Wise (optimized cumulative method)
        # Sort scores ascending for binary search; precompute cumulative positives
        order_asc = np.argsort(s)
        scores_asc = s[order_asc]
        labels_asc = y[order_asc].astype(int)
        cum_pos = np.cumsum(labels_asc)
        total_pos = int(cum_pos[-1])
        total = len(s)
        total_neg = total - total_pos

        precision = []; recall = []; f1 = []; fpr = []
        # We iterate over threshold list (already reduced). For each threshold t, predicted positives are indices with score >= t.
        for th in thresholds:
            # first index where score >= th
            idx_left = np.searchsorted(scores_asc, th, side='left')
            # predicted positives count
            pred_pos = total - idx_left
            if pred_pos == 0:
                TP_t = 0
                FP_t = 0
            else:
                # positives among predicted positives = total_pos - positives before idx_left
                pos_before = cum_pos[idx_left - 1] if idx_left > 0 else 0
                TP_t = total_pos - pos_before
                FP_t = pred_pos - TP_t
            FN_t = total_pos - TP_t
            TN_t = total_neg - FP_t
            p = TP_t / (TP_t + FP_t + 1e-8)
            r = TP_t / (TP_t + FN_t + 1e-8)
            precision.append(p); recall.append(r)
            f1.append(2 * (p * r) / (p + r + 1e-8))
            fpr.append(FP_t / (FP_t + TN_t + 1e-8))
        idx = int(np.argmax(f1))
        pr_auc_val = float(auc(recall, precision)) if len(precision) > 1 else float('nan')
        roc_auc_val = float(auc(fpr, recall)) if len(fpr) > 1 else float('nan')
        out['point_wise'] = {
            'threshold': float(thresholds[idx]),
            'precision': float(precision[idx]),
            'recall': float(recall[idx]),
            'f1': float(f1[idx]),
            'pr_auc': pr_auc_val,
            'roc_auc': roc_auc_val,
        }

        # Released Point-Wise (nest_length)
        thresholds_rel = thresholds  # reuse same threshold grid
        s_seq = _create_sequences(s, opts.nest_length, opts.step)
        y_seq = _create_sequences(y, opts.nest_length, opts.step)
        TP = []; TN = []; FP = []; FN = []
        precision = []; recall = []; f1 = []; fpr = []
        for th in thresholds_rel:
            TP_t = TN_t = FP_t = FN_t = 0
            for t in range(len(s_seq)):
                true_anom = set(np.where(y_seq[t] == 1)[0])
                pred_anom = set(np.where(s_seq[t] > th)[0])
                if len(pred_anom) > 0 and len(pred_anom.intersection(true_anom)) > 0:
                    TP_t += 1
                elif len(pred_anom) == 0 and len(true_anom) == 0:
                    TN_t += 1
                elif len(pred_anom) > 0 and len(true_anom) == 0:
                    FP_t += 1
                elif len(pred_anom) == 0 and len(true_anom) > 0:
                    FN_t += 1
            TP.append(TP_t); TN.append(TN_t); FP.append(FP_t); FN.append(FN_t)
        for i in range(len(thresholds_rel)):
            p = TP[i] / (TP[i] + FP[i] + 1e-7)
            r = TP[i] / (TP[i] + FN[i] + 1e-7)
            precision.append(p); recall.append(r)
            f1.append(2 * (p * r) / (p + r + 1e-7))
            fpr.append(FP[i] / (FP[i] + TN[i] + 1e-7))
        idx = int(np.argmax(f1))
        out['released_point_wise'] = {
            'threshold': float(thresholds_rel[idx]),
            'precision': float(precision[idx]),
            'recall': float(recall[idx]),
            'f1': float(f1[idx]),
            'pr_auc': float(auc(recall, precision)),
            'roc_auc': float(auc(fpr, recall)),
        }

        # TranAD-adjusted
        auc_val, p_raw, r_raw, f1_raw, p_adj, r_adj, f1_adj, th_adj = get_threshold_tranad(
            labels=y.astype(int), scores=s, print_or_not=False
        )
        out['tranad_adjusted'] = {
            'threshold': float(th_adj),
            'precision': float(p_adj),
            'recall': float(r_adj),
            'f1': float(f1_adj),
            'pr_auc': float('nan'),
            'roc_auc': float(auc_val),
        }
        return out, n

    # Build labels once (use raw 1D label sequence, not windowed y_test which is 3D)
    if opts.dataset in ('PSM','SMD','SMAP','MSL'):
        data_dict = load_dataset_by_name(opts.dataset, seq_length=opts.seq_length, stride=1)
        labels_full = np.asarray(data_dict['label_seq'][0]).astype(int)
        if labels_full.ndim != 1:
            # Safety flatten
            labels_full = labels_full.reshape(-1)
        print(f"[DEBUG] Loaded label_seq length: {len(labels_full)} (ndim={labels_full.ndim})")
    else:
        print(f"Dataset {opts.dataset} not implemented yet")
        return

    # Decide which modes to run
    wanted_modes = []
    mode = (opts.mode or 'all').lower()
    if mode == 'all':
        wanted_modes = ['time', 'freq', 'fused']
    else:
        if mode not in ['time', 'freq', 'fused']:
            print(f"Unknown mode {mode}, defaulting to fused")
            wanted_modes = ['fused']
        else:
            wanted_modes = [mode]

    # Prepare output accumulator
    rows = []
    for m in wanted_modes:
        if m == 'time':
            scores = time_only_as
        elif m == 'freq':
            scores = freq_only_as
        else:
            scores = final_as

        res, n_used = evaluate_scores(scores, labels_full)

        print(f"\n=== Evaluation Mode: {m} (N={n_used}) ===")
        for k, v in res.items():
            print(f"-- {k} --")
            print("Threshold: {:.6f}".format(v['threshold']))
            print("Precision : {:0.4f}, Recall : {:0.4f}, F1 : {:0.4f}".format(v['precision'], v['recall'], v['f1']))
            if not (np.isnan(v['pr_auc'])):
                print("PR-AUC : {:0.4f}".format(v['pr_auc']))
            print("ROC-AUC : {:0.4f}".format(v['roc_auc']))

            rows.append({
                'dataset': opts.dataset,
                'data_num': opts.data_num,
                'mode': m,
                'method': k,
                'seq_length': opts.seq_length,
                'nest_length': opts.nest_length,
                'step': opts.step,
                'thresh_num': opts.thresh_num,
                'threshold': v['threshold'],
                'precision': v['precision'],
                'recall': v['recall'],
                'f1': v['f1'],
                'pr_auc': v['pr_auc'],
                'roc_auc': v['roc_auc'],
                'n_used': n_used,
            })

    # Optionally save CSV
    if not opts.no_save_csv:
        results_dir = Path(opts.results_dir)
        results_dir.mkdir(parents=True, exist_ok=True)
        out_path = results_dir / f"{opts.dataset}_{opts.data_num}_results.csv"
        df_out = pd.DataFrame(rows)
        # Append if exists, else create
        if out_path.exists():
            old = pd.read_csv(out_path)
            df_out = pd.concat([old, df_out], ignore_index=True)
        df_out.to_csv(out_path, index=False)
        print(f"\nSaved consolidated results to: {out_path}")

    print("\n=== FL Evaluation Complete ===")


def main():
    """Main entry point for FL evaluation."""
    parser = argparse.ArgumentParser(description='FL-Compatible Evaluation for DualTF')

    # Settings
    parser.add_argument('--thresh_num', type=int, default=1000,
                        help='Number of thresholds for evaluation')
    parser.add_argument('--seq_length', type=int, default=100,
                        help='Sequence length used during training')
    parser.add_argument('--nest_length', type=int, default=25,
                        help='Nested sequence length')
    parser.add_argument('--step', type=int, default=1,
                        help='Step size for sequence creation')
    parser.add_argument('--dataset', type=str, default='PSM',
                        help='Dataset name')
    parser.add_argument('--form', type=str, default=None,
                        help='Form for NeurIPSTS dataset')
    parser.add_argument('--data_num', type=int, default=0,
                        help='Data number')
    parser.add_argument('--mode', type=str, default='all',
                        help='Which scores to evaluate: time|freq|fused|all')
    parser.add_argument('--results_dir', type=str, default='./eval_results',
                        help='Directory to save consolidated CSV results')
    parser.add_argument('--no_save_csv', action='store_true',
                        help='If set, do not write consolidated CSV')
    parser.add_argument('--verbose', action='store_true',
                        help='Print loaded arrays for inspection')

    opts = parser.parse_args()

    # Display settings
    if opts.dataset == 'NeurIPSTS' and opts.form:
        print(f"Dataset: {opts.dataset}\nForm: {opts.form}\nSeq_length: {opts.seq_length}\nNest_length: {opts.nest_length}")
    else:
        print(f"Dataset: {opts.dataset}\nNum: {opts.data_num}\nSeq_length: {opts.seq_length}\nNest_length: {opts.nest_length}")

    # Run evaluation
    total_evaluation(opts)


if __name__ == '__main__':
    main()